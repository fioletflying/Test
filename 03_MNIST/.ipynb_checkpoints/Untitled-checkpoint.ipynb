{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /path/to/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting /path/to/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /path/to/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /path/to/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Training data size: 55000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/path/to/MNIST_data\",one_hot = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 55000\n",
      "Validating data size 5000\n",
      "Test data size: 10000\n",
      "Training data: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3803922  0.37647063 0.3019608\n",
      " 0.46274513 0.2392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3529412\n",
      " 0.5411765  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687\n",
      " 0.9215687  0.9843138  0.9843138  0.9725491  0.9960785  0.9607844\n",
      " 0.9215687  0.74509805 0.08235294 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.7411765  0.09019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8862746  0.9960785  0.81568635 0.7803922  0.7803922  0.7803922\n",
      " 0.7803922  0.54509807 0.2392157  0.2392157  0.2392157  0.2392157\n",
      " 0.2392157  0.5019608  0.8705883  0.9960785  0.9960785  0.7411765\n",
      " 0.08235294 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.32156864\n",
      " 0.0509804  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.13333334 0.8352942  0.9960785  0.9960785  0.45098042 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.32941177\n",
      " 0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32941177 0.9960785  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4156863  0.6156863  0.9960785  0.9960785  0.95294124 0.20000002\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.09803922\n",
      " 0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.26666668 0.4666667  0.86274517 0.9960785  0.9960785\n",
      " 0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
      " 0.9960785  0.5568628  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14509805 0.73333335 0.9921569\n",
      " 0.9960785  0.9960785  0.9960785  0.8745099  0.8078432  0.8078432\n",
      " 0.29411766 0.26666668 0.8431373  0.9960785  0.9960785  0.45882356\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
      " 0.34901962 0.12156864 0.         0.         0.         0.\n",
      " 0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.6627451  0.9960785\n",
      " 0.6901961  0.24313727 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.18823531 0.9058824  0.9960785\n",
      " 0.9176471  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.07058824 0.48627454 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.32941177 0.9960785  0.9960785  0.6509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.54509807\n",
      " 0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.8235295  0.9803922  0.9960785  0.65882355\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9490197  0.9960785  0.93725497 0.22352943 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
      " 0.3372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01960784 0.8078432  0.96470594 0.6156863  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01568628 0.45882356\n",
      " 0.27058825 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Training data label: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "x shape: (100, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data size:\",mnist.train.num_examples)\n",
    "print(\"Validating data size\",mnist.validation.num_examples)\n",
    "print(\"Test data size:\",mnist.test.num_examples)\n",
    "\n",
    "print(\"Training data:\",mnist.train.images[0])\n",
    "print(\"Training data label:\",mnist.train.labels[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (100, 784)\n",
      "Y shape: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "xs,ys = mnist.train.next_batch(batch_size)\n",
    "print(\"x shape:\",xs.shape)\n",
    "print(\"Y shape:\",ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data\\train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-3-00301088d907>:65: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n",
      "0 steps, validation accusing average model is 0.1046\n",
      "1000 steps, validation accusing average model is 0.9762\n",
      "2000 steps, validation accusing average model is 0.981\n",
      "3000 steps, validation accusing average model is 0.9834\n",
      "4000 steps, validation accusing average model is 0.9836\n",
      "5000 steps, validation accusing average model is 0.9854\n",
      "6000 steps, validation accusing average model is 0.9856\n",
      "7000 steps, validation accusing average model is 0.9856\n",
      "8000 steps, validation accusing average model is 0.9838\n",
      "9000 steps, validation accusing average model is 0.985\n",
      "10000 steps, validation accusing average model is 0.986\n",
      "11000 steps, validation accusing average model is 0.9858\n",
      "12000 steps, validation accusing average model is 0.9854\n",
      "13000 steps, validation accusing average model is 0.9858\n",
      "14000 steps, validation accusing average model is 0.9852\n",
      "15000 steps, validation accusing average model is 0.9846\n",
      "16000 steps, validation accusing average model is 0.9854\n",
      "17000 steps, validation accusing average model is 0.9848\n",
      "18000 steps, validation accusing average model is 0.985\n",
      "19000 steps, validation accusing average model is 0.9854\n",
      "20000 steps, validation accusing average model is 0.9848\n",
      "21000 steps, validation accusing average model is 0.9846\n",
      "22000 steps, validation accusing average model is 0.9852\n",
      "23000 steps, validation accusing average model is 0.9846\n",
      "24000 steps, validation accusing average model is 0.9844\n",
      "25000 steps, validation accusing average model is 0.9848\n",
      "26000 steps, validation accusing average model is 0.9852\n",
      "27000 steps, validation accusing average model is 0.9858\n",
      "28000 steps, validation accusing average model is 0.985\n",
      "29000 steps, validation accusing average model is 0.9848\n",
      "30000 steps, test accmodel is 0.9832\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 输入的节点数量，等于图片的像素\n",
    "INPUT_NODE = 784\n",
    "# 输出的节点数（0-9）10个数字\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "# 隐藏层的网络结构，500个节点\n",
    "LAYER1_NODE = 500\n",
    "# 一个batch 中的训练数据的个数，\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 基础的学习率\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "# 学习率的衰减\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "# 正则化项的损失函数中的系数\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "# 训练步数\n",
    "TRAINING_STEPS = 30000\n",
    "# 滑动平均衰减率\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "# 计算前向传播的结果\n",
    "def inference0(input_tensor,avg_class,weights1,biases1,weights2,biases2):\n",
    "    if avg_class == None:\n",
    "        # relu激活函数\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor,weights1)+biases1)\n",
    "        # 计算输出前向传播的结果，这里没有加入softmax函数\n",
    "        # 因为损失函数会一并计算softmaxe\n",
    "        return tf.matmul(layer1,weights2) + biases2\n",
    "    else: # 计算滑动平均参数\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor,avg_class.average(weights1))+avg_class.average(biases1))\n",
    "        return tf.matmul(layer1,avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "\n",
    "    \n",
    "def inference(input_tensor,reuse = False):\n",
    "    with tf.variable_scope('layer1',reuse = reuse):\n",
    "        weights = tf.get_variable(\"weights\",[INPUT_NODE,LAYER1_NODE],\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev = 0.1))\n",
    "        biases = tf.get_variable(\"biases\",[LAYER1_NODE],\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor,weights)+biases)\n",
    "        \n",
    "    with tf.variable_scope('layer2',reuse = reuse):\n",
    "        weights = tf.get_variable(\"weights\",[LAYER1_NODE,OUTPUT_NODE],\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev = 0.1))\n",
    "        biases = tf.get_variable(\"biases\",[OUTPUT_NODE],\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "        layer1 = tf.matmul(layer1,weights)+biases\n",
    "    \n",
    "# 训练模型\n",
    "def train(mnist):\n",
    "    \n",
    "    # 输入数据\n",
    "    x  = tf.placeholder(tf.float32,[None,INPUT_NODE],name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32,[None,OUTPUT_NODE],name = 'y-input')\n",
    "    \n",
    "    # 生成隐藏参数\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE,LAYER1_NODE],stddev = 0.1))\n",
    "    biase1 = tf.Variable(tf.constant(0.1,shape=[LAYER1_NODE]))\n",
    "    # 生成输出参数\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE,OUTPUT_NODE],stddev = 0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1,shape =[OUTPUT_NODE]))\n",
    "    \n",
    "    # 计算前向传播的结果\n",
    "    y = inference(x,None,weights1,biase1,weights2,biases2)\n",
    "    \n",
    "    # 定义存储轮数的变量，指定不可训练的变量\n",
    "    global_step = tf.Variable(0,trainable=False)\n",
    "    \n",
    "    # 给定滑动平均衰减率和训练轮数的变量\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    # \n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    # 滑动平均之后的前向传播结果\n",
    "    average_y = inference(x,variable_averages,weights1,biase1,weights2,biases2)\n",
    "    \n",
    "    # 计算交叉熵\n",
    "    corss_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y,labels=tf.argmax(y_,1))\n",
    "    # 计算当前batch中所有样例的交叉熵的平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(corss_entropy)\n",
    "    \n",
    "    # 计算L2 正则化损失函数\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    # 计算模型的正则化损失\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    # 总损失需要加上正则化的损失\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    # 设置衰减的学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                LEARNING_RATE_BASE, #基础学习率\n",
    "                global_step, # 当前迭代的轮数\n",
    "                mnist.train.num_examples / BATCH_SIZE, #需要迭代的次数\n",
    "                LEARNING_RATE_DECAY # 学习率衰减的速度\n",
    "                )\n",
    "    \n",
    "    \n",
    "    # 优化算法来优化损失函数\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "\n",
    "    with tf.control_dependencies([train_step,variable_averages_op]):\n",
    "        train_op = tf.no_op(name = 'train')\n",
    "\n",
    "    correct_prediction = tf.equal(tf.arg_max(average_y,1),tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        validate_feed = {x:mnist.validation.images,\n",
    "                        y_:mnist.validation.labels}\n",
    "\n",
    "        test_feed = {x:mnist.test.images,y_:mnist.test.labels}\n",
    "\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i % 1000 == 0:\n",
    "                validate_acc = sess.run(accuracy,feed_dict=validate_feed)\n",
    "                print(\"%d steps, validation acc\" \"using average model is %g\" %(i,validate_acc))\n",
    "\n",
    "            xs,ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op,feed_dict={x:xs,y_:ys})\n",
    "\n",
    "        test_acc = sess.run(accuracy,feed_dict = test_feed)\n",
    "        print(\"%d steps, test acc\" \"model is %g\" %(TRAINING_STEPS,test_acc))\n",
    "    \n",
    "    \n",
    "\n",
    "def main(argv = None):\n",
    "    mnist = input_data.read_data_sets(\"/tmp/data\",one_hot = True)\n",
    "    train(mnist)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  变量的管理\n",
    "\n",
    " TensorFlow 提供了通过变量名称来创建或者获取一个变量 的机制 。 通过这个机制，在不同的函数中可．以直接通过变量 的名字来使用变量 ，而不需要将变盘通过参数的形式到处传递 。 \n",
    "\n",
    "TensorFlow 中通过变 量 名称获取变量的机制主要是通过 \n",
    "- `tf.get_variable`\n",
    "    - 函数来创建或者获取变量。\n",
    "    - 必须指定变量名称的参数，会根据这个名字去创建或者获取变量 。 \n",
    "- `tf.variable_scope`\n",
    "\n",
    "\n",
    "\n",
    "![initializer.png](imgs/initializer.png)\n",
    "\n",
    " 如果需要通过tf.get_variable 获取一个已经创建的变量，需要通过 tf.variable_scope 函数来生成一个上下文管理器，并明确指定在这个上下文管理器中，tf.get_variable将直接获取己经生成的变量。\n",
    " tf.variable_scope 函数可以控制 tf.get_variable 函数的语义，函数使用参数 reuse=True 生成上下文管理器时，这个上下文管理器内所有的 tf.get_variable函数会直接获取己经创建的变量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'v2_1:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'v1:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# tf.getvariable\n",
    "import tensorflow as tf\n",
    "v1 = tf.get_variable(\"v1\",shape=[1],initializer=tf.constant_initializer(1.0))\n",
    "v2 = tf.Variable(tf.constant(1.0,shape=[1]),name = \"v2\")\n",
    "print(v2)\n",
    "print(v1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    v = tf.get_variable(\"v\",[1],initializer=tf.constant_initializer(1.0))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#在生成上下文管理器，将参数reuse 设置为True,就可以直接获得已经声明的参数\n",
    "with tf.variable_scope(\"foo\",reuse = True):\n",
    "    v1 = tf.get_variable(\"v\",[1])\n",
    "    print (v == v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
