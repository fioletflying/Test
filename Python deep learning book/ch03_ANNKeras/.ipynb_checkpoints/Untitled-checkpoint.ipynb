{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测波士顿房价-回归问题案例\n",
    "\n",
    "<p style=\"text-indent:2em\">\n",
    "**问题描述**：回归问题是机器学习常见的问题，它预测一个连续值而不是离散的标签，例如，根据气象数据预测明天的气温，或者根据软件说明书预测完成软件项目所需要的时间\n",
    "</p>\n",
    "\n",
    "注意的地方： logistic 回归不是回归算法，而是分类算法。祥见[logistic 回归](https://baike.baidu.com/item/logistic%E5%9B%9E%E5%BD%92/2981575?fr=aladdin)\n",
    "\n",
    "### 数据加载\n",
    "\n",
    "<p style=\"text-indent:2em\">\n",
    "   本节将要预测 20 世纪 70 年代中期波士顿郊区房屋价格的中位数，已知当时郊区的一些数据点，比如犯罪率、当地房产税率等。  \n",
    " </p>\n",
    " \n",
    "\n",
    "&emsp;&emsp;只有 506 个，分为 404 个训练样本和 102 个测试样本。输入数据的每个特征（比如犯罪率）都有不同的取值范围。，有些特性是比例，取值范围为 0~1；有的取值范围为 1~12；还有的取值范围为 0~100，\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data,train_targets),(test_data,test_targets) \\\n",
    "= boston_housing.load_data(\"boston_housing.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(102, 13)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "#print(train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据归一化\n",
    "\n",
    "&emsp;&emsp;由于特征数据的取值范围不同，这样导致网络训练的难度，所以需要对特征数据进行同意的标准化。一般采用的方法：\n",
    "- 将数据归一化到(0,1),也就是平均值为0，标准差为1。\n",
    "- 具体做法是：减去平均值，再除以标准差\n",
    "coding实现如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean = train_data.mean(axis = 0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data/=std\n",
    "\n",
    "# 测试数据的标准化，也只能使用训练数据的mean,std\n",
    "test_data -= mean\n",
    "test_data/=std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建网络\n",
    "\n",
    "一般来说，训练数据越少，过拟合会越严重，而较小的网络可以降低过拟合。\n",
    "网络的主体：\n",
    "   - 两个中间层，每层都有 64 个隐藏单元，使用relu作为激活函数；\n",
    "   - 第三层输出一个标量，是一个线性层，不需要激活函数这样可以实现任意值的预测。\n",
    "   \n",
    "注意的点：\n",
    "- loss函数：用的是 mse 损失函数，即均方误差（MSE，mean squared error），预测值与目标值之差的平方。这是回归问题常用的损失函数；\n",
    "- 监控一个新指标：：平均绝对误差（MAE，mean absolute error）。它是预测值与目标值之差的绝对值。\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64,activation='relu',\n",
    "                          input_shape =(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64,activation='relu'))\n",
    "    # 注意没有激活层，是一个线性层，因为回归的是一个标量\n",
    "    model.add(layers.Dense(1))\n",
    "    # mse:均方误差\n",
    "    # mae:平均绝对误差\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用`K`折验证法\n",
    "\n",
    "#### 何时使用K折验证法  \n",
    "\n",
    "当数据量较小时，验证集的数据也较小，这样到时验证的分数波动很大，也就时方差很大，这样就无法做出有效的评估。\n",
    "\n",
    "#### 什么时K折验证法\n",
    "\n",
    "将可用数据划分为 K个分区（K 通常取 4 或 5），实例化 K 个相同的模型，将每个模型在 K-1 个分区上训练，并在剩下的一个分区上进行评估。具体如下图：  \n",
    "\n",
    "![3折交叉验证](imgs/05.jpg)\n",
    "\n",
    "在coding使用一个numpy中的函数，也就是数据堆叠。\n",
    "[concatenate的使用](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.concatenate.html)\n",
    "\n",
    "每次运行模型得到的验证分数有很大差异，从 2.6 到 3.2 不等。平均分数（3.0）是比单一分数更可靠的指标——这就是 K 折交叉验证的关键。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold # 0\n",
      "Processing fold # 1\n",
      "Processing fold # 2\n",
      "Processing fold # 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('Processing fold #',i)\n",
    "    val_data = train_data[i * num_val_samples \n",
    "                        : (i+1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples \n",
    "                        : (i+1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "        train_data[(i+1) * num_val_samples:]],\n",
    "        axis = 0)\n",
    "    \n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i*num_val_samples],\n",
    "        train_targets[(i+1)*num_val_samples:]],\n",
    "        axis = 0)\n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data,\n",
    "             partial_train_targets,\n",
    "             epochs = num_epochs,\n",
    "             batch_size = 1,\n",
    "             verbose = 0)\n",
    "    val_mse,val_mae = model.evaluate(val_data,\n",
    "                                    val_targets,\n",
    "                                    verbose=0)\n",
    "    all_scores.append(val_mae)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.8786396956679843, 3.3748388007135675, 4.058458493487669, 3.562984341441995]\n",
      "mean scorces: 3.468730332827804\n"
     ]
    }
   ],
   "source": [
    "print(all_scores)\n",
    "print(\"mean scorces:\",np.mean(all_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold # 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c595de1c3ac1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m              \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m              verbose = 0)\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mmae_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_mean_absolute_error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mall_mae_histories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmae_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 500\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('Processing fold #',i)\n",
    "    val_data = train_data[i * num_val_samples \n",
    "                        : (i+1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples \n",
    "                        : (i+1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "        train_data[(i+1) * num_val_samples:]],\n",
    "        axis = 0)\n",
    "    \n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i*num_val_samples],\n",
    "        train_targets[(i+1)*num_val_samples:]],\n",
    "        axis = 0)\n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data,\n",
    "                 partial_train_targets,\n",
    "                 validation_data = (val_data,val_targets),\n",
    "                 epochs = num_epochs,\n",
    "                 batch_size = 1,\n",
    "                 verbose = 0)\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    all_mae_histories.append(mae_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制验证分数\n",
    "import matplotlib.pyplot as plt \n",
    " \n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history) \n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Validation MAE') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.9): \n",
    "  smoothed_points = [] \n",
    "  for point in points: \n",
    "    if smoothed_points: \n",
    "      previous = smoothed_points[-1] \n",
    "      smoothed_points.append(previous * factor + point * (1 - factor)) \n",
    "    else: \n",
    "      smoothed_points.append(point) \n",
    "  return smoothed_points \n",
    " \n",
    "smooth_mae_history = smooth_curve(average_mae_history[10:]) \n",
    " \n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history) \n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Validation MAE') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
