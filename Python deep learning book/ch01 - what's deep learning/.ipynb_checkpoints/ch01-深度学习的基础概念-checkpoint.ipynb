{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TOC]\n",
    "\n",
    "# 深度学习的基础概念\n",
    "\n",
    "## 人工智能、机器学习与深度学习\n",
    "\n",
    "![人工智能、机器学习与深度学习](imgs/01.png)\n",
    "\n",
    "### 人工智能\n",
    "\n",
    "人工智能的概念： 努力将通常由人类完成的智力任务自动化。\n",
    "\n",
    "### 机器学习\n",
    "\n",
    "机器学习系统是训练出来的，而不是明确地用程序编写出来的。\n",
    "机器学习：在预先定义好的可能性空间中，利用反馈信号的指引来寻找输入数据的有用表示。\n",
    "\n",
    "### 深度学习\n",
    "机器学习和深度学习的核心问题在于有意义地变换数据，换句话说，在于学习输入数据的有用表示（representation）——这种表示可以让数据更接近预期输出。这一概念的核心在于以一种不同的方式来查看数据（即表征数据或将数据编码）。\n",
    "\n",
    "深度学习是机器学习的一个分支领域：它是从数据中学习表示的一种新方法，强调从连续的层（layer）中进行学习，这些层对应于越来越有意义的表示。\n",
    "\n",
    "“深度学习”中的**“深度”**指的并不是利用这种方法所获取的更深层次的理解，而是指一系列**连续的表示层**。\n",
    "\n",
    "数据模型中包含多少层，这被称为模型的深度（depth），这些分层表示几乎总是通过叫作神经网络（neural network）的模型来学习得到的。\n",
    "\n",
    "你可以将深度网络看作多级信息蒸馏操作：信息穿过连续的过滤器，其纯度越来越高（即对任务的帮助越来越大）。\n",
    "\n",
    "### 深度学习工作原理\n",
    "\n",
    "- 权重： 神经网络中每层对输入数据所做的具体操作其本质是一串数字，为该层的**参数**\n",
    "- 学习：为神经网络的所有层找到一组权重值\n",
    "\n",
    "![权重的参数化](imgs/02.png)\n",
    "\n",
    "\n",
    "- 损失函数(loss function): 想要控制神经网络的输出，就需要能够衡量该输出与预期值之间的距离。\n",
    "\n",
    "![损失函数](imgs/03.png)\n",
    "\n",
    "- 优化器(optimizer):利用这个距离值作为反馈信号来对权重值进行微调,它实现了所谓的反向传播（backpropagation）算法\n",
    "\n",
    "![优化器](imgs/05.png)\n",
    "\n",
    "## 机器学习简史\n",
    "\n",
    "### 概率建模\n",
    "\n",
    "- 朴素贝叶斯是一类基于应用贝叶斯定理的机器学习分类器，它假设输入数据的特征都是独立的。\n",
    "-  logistic 回归是一种分类算法，而不是回归算法。\n",
    "\n",
    "### 早期神经网络\n",
    "\n",
    "- 反向传播算法:一种利用梯度下降优化来训练一系列参数化运算链的方法\n",
    "- 贝尔实验室于 1989 年第一次成功实现了神经网络的实践应用\n",
    "- LeNet 的网络，在 20 世纪 90 年代被美国邮政署采用，用于自动读取信封上的邮政编码。\n",
    "\n",
    "### 核方法\n",
    "\n",
    "- SVM:通过在属于两个不同类别的两组数据点之间找到良好决策边界\n",
    "\n",
    "### 决策树、随机森林与梯度提升机\n",
    "\n",
    "- 决策树（decision tree）是类似于流程图的结构，可以对输入数据点进行分类或根据给定输入来预测输出值\n",
    "- 随机森林（random forest）算法:构建许多决策树，然后将它们的输出集成在一起\n",
    "- 梯度提升机（gradient boosting machine）也是将弱预测模型）集成的机器学习技术。它使用了梯度提升方法，通过迭代地训练新模型来专门解决之前模型的弱点，从而改进任何机器学习模型的效果。\n",
    "\n",
    "### 回到神经网络\n",
    "\n",
    "- 2012 年，当年 Hinton 小组参加了每年一次的大规模图像分类挑战赛 ImageNet。深度卷积神经网络（convnet）\n",
    "\n",
    "\n",
    "## 深度学习为什么现在火\n",
    "\n",
    "### 硬件\n",
    "-  GPU 的快速发展\n",
    "- 。2016 年，Google 在其年度 I/O 大会上展示了张量处理器（TPU）项目，它的速度比最好的 GPU 还要快 10 倍，\n",
    "###  数据集和基准\n",
    "-  ImageNet 数据集\n",
    "\n",
    "### 算法上的改进\n",
    "- 在 2014 年、2015 年和 2016 年，人们发现了更先进的有助于梯度传播的方法\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
