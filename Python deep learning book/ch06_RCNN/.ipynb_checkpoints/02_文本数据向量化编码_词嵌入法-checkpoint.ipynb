{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词嵌入（word embedding）\n",
    "\n",
    "one-hot编码的特征：\n",
    "- 二进制的、稀疏的（绝大部分元素都是 0）\n",
    "- 维度很高（维度大小等于词表中的单词个数）\n",
    "- 硬编码（信息的关联度几乎为0）\n",
    "\n",
    "词嵌入编码的特征：\n",
    "-  密集 （更多的信息塞入更低的维度）\n",
    "- 维度较低（ 256、512 或 1024）\n",
    "- 从数据中学习得到（信息的关联度高）\n",
    "\n",
    "所以词嵌入的向量编码具有更多的信息。\n",
    "\n",
    "![两种编码的对比](imgs/02.jpg)\n",
    "\n",
    "#### 获得词嵌入编码的两种方法\n",
    "\n",
    "- 在完成主任务（比如文档分类或情感预测）的同时学习词嵌入。，一开始是随机的词向量，然后对这些词向量进行学习，其学习方式与学习神经网络的权重相同；\n",
    "- 预训练词嵌入（pretrained word embedding）,利用预先计算好词嵌入加载到自己的模型中。\n",
    "\n",
    "###  利用 Embedding 层学习词嵌入\n",
    "\n",
    "所谓的词嵌入，就是将单个的词与一个密集向量进行关联。这里的密集向量也就是上面提到的维度数，例如256，512等等向量。这里就是需要我们怎么来定义这个密集向量，说的更加简单点：这个向量空间由什么组成，怎么组成？\n",
    "\n",
    "再解释这个问题之前，我们先来理解一下，词向量之间的关系如何来表示。\n",
    "\n",
    "#### 词向量之间的几何关系\n",
    "\n",
    "每个词与词之间是存在一定的语义逻辑关系的，比如 cat（猫）、dog（狗）、wolf（狼）\n",
    "和 tiger（虎），这四个词语。\n",
    "- 从 cat 到 tiger 的向量与从 dog 到 wolf 的向量相等，这个向量可以被解释为“从宠物到野生动物”向量\n",
    "- 从 dog 到 cat 的向量与从 wolf 到 tiger 的向量也相等，它可以被解释为“从犬科到猫科”向量。\n",
    "\n",
    "词嵌入的作用是将人类的语言映射到几何空间中，词与词之间的语义关系就是通过几何距离来表示。例如：表示不同事物的词被嵌入到相隔很远的点，而相关的词则更加靠近（“花” 与“植物”，“狗” 与 “动物” 靠的近一些），除了距离以外，还有嵌入空间中的特定方向也是有意义。例如上面的哪个例子。\n",
    "\n",
    "真实的词嵌入空间，通常是由一些比较综合的词组成的向量空间，例如：“性别”向量和“复数”向量，但是再人类语言中，无法完美的映射一个理想的词嵌入空间，因为语言和文化不一样。这里如果一定要找一个合理的词嵌入空间的化，要根据具体的任务来获得。比如英语电影评论情感分析模型的完美词嵌入空间，与科技论文的模型的完美词嵌入空间就不一样。  \n",
    "\n",
    "合理的做法是对每个新任务都学习一个新的嵌入空间，反向传播让这种学习变得很简单。通过Keras API可以很方便得到。\n",
    "\n",
    "####  keras 中 Embedding 层的理解\n",
    "\n",
    " Embedding 层理解为一个字典，将整数索引（表示特定单词）映射为密集向量。\n",
    " 它接收整数作为输入，并在内部字典中查找这些整数，然后返回相关联的向量。\n",
    " ![Embedding 层](imgs/03.jpg)\n",
    " \n",
    " Embedding 层的输入是一个二维整数张量，其形状为 (samples,  sequence_length)，每 个 元 素 是 一 个 整 数 序 列。   \n",
    "  对 于 前 一 个 例 子 中 的Embedding 层，你可以输入形状为 (32,  10)（32 个长度为 10 的序列组成的批量）或 (64, 15)（64 个长度为 15 的序列组成的批量）的批量。  \n",
    "  \n",
    "这 个 Embedding 层 返 回 一 个 形 状 为 (samples,  sequence_length,  embedding_dimensionality) 的三维浮点数张量   \n",
    "\n",
    "将一个 Embedding 层实例化时，它的权重（即标记向量的内部字典）最开始是随机的，与其他层一样。在训练过程中，利用反向传播来逐渐调节这些词向量，改变空间结构以便下游模型可以利用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个 Embedding 层实例化\n",
    "from keras.layers import Embedding\n",
    "#  两个参数：\n",
    "# 标记的个数（这里是 1000，即最大单词索引 +1）\n",
    "# 嵌入的维度（这里是 64）\n",
    "embedding_layer = Embedding(1000,64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   IMDB 电影评论情感预测模型\n",
    "\n",
    "- 准备数据：\n",
    "    - 将电影评论限制为前 10 000 个最常见的单词，\n",
    "    - 评论长度限制为20个单词，\n",
    "    - 每个单词学习一个8维度向量\n",
    "- 将输入的整数序列（二维整数张量）转换为嵌入序列（三维浮点数张量）\n",
    "- 训练一个 Dense 层用于分类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e7ecb004c9fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m  \u001b[1;31m# 作为特征的单词个数,前 10 000 个最常见的单词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'preprocessing'"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.layers import preprocessing\n",
    "\n",
    " # 作为特征的单词个数,前 10 000 个最常见的单词\n",
    "max_features = 10000 \n",
    "# 评论长度限制为20个单词,在这么多单词后截断文本（\n",
    "maxlen = 20\n",
    "\n",
    "# 将数据加载为整数列表\n",
    "(x_train,y_train),(x_test,y_test) = imdb.load_data(\n",
    "    num_words=max_features) \n",
    "\n",
    "#将整数列表转换成形状为(samples, maxlen) 的二维整数张量\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=maxlen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
