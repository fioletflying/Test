{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习与深层神经网络\n",
    "\n",
    "\n",
    "深度学习的概念：**一类通过多层非线性变换对高复杂性数据建模算法的合集**\n",
    "- “多层非线性变换”\n",
    "\n",
    "###  线性模型的局限性\n",
    "\n",
    "只通过线性变换，任意层的全连接神经网络和单层神经网络模型的表达能力没有任何区别，而且它们都是线性模型。 因为可以通过矩阵变换达到多层的效果。\n",
    "\n",
    "在深度学习的定义中特意强调它的目的为解决更加复杂的问题 。 所谓复杂问题，至少是无法通过直线（或者高维空间的平面）划分的 。\n",
    "\n",
    "### 激活函数实现去线性化\n",
    "\n",
    "将每一个神经元（也就是神经网络中的节点）的输出通过一个非线性函数，那么整个神经网络的模型也就不再是线性的了。这个非线性函数就是激活函数。\n",
    "\n",
    "![activatefuncs.png](imgs\\activatefuncs.png)\n",
    "\n",
    "**TensorFlow中的激活函数**\n",
    "-`tf.nn.relu`\n",
    "- `tf.nn.sigmoid`\n",
    "- `tf.nn.tanh`\n",
    "- TensorFlow 也支持使用自己定义的激活函数\n",
    "\n",
    "\n",
    "### 多层网络解决异或运算\n",
    "\n",
    "- 单层感知机无法模拟异或运算的功能。\n",
    "- 深层神经网络实际上有组合特征提取的功能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数定义\n",
    "\n",
    "   分类问题和回归问题是监督学习的两大种类。这一节将分别介绍分类问题和回归问题中使用到的经典损失函数。分类问题希望解决的是将不同的样本分到事先定义好的类别中。\n",
    "   \n",
    "### 经典损失函数\n",
    "   \n",
    "   交叉熵（ cross entropy ）是常用的评判方法之一 。交叉：脑刻画了两个概率分布之间的距离 ， 它是分类问题中使用比较广的一种损失函数。\n",
    "   \n",
    "   \n",
    "   - 交叉姻是一个信息论中的概念，它原本是用来估算平均编码长度的。\n",
    "   \n",
    "两个概率分布 p 和 q ， 通过 q 来表示 p 的交叉煽\n",
    "   \n",
    "$ H(p,q) = - \\sum_x{p(x)log{q(x)}} $\n",
    "\n",
    "- Softmax 回归就是一个非常常用的方法。\n",
    "- 将神经网络的输出变成一个概率分布\n",
    "- 那么经过 Softmax 回归处理之后的输出为 ：\n",
    "- $softmax(y)_{i} = {y^{'}_{i}}=\\frac{e^{yi}}{\\sum^{n}_{j=1}e^{yj}}$\n",
    "- 这个新的输出可以理解为经过神经网络的推导， 一个样例为不同类别的概率分别是多大。\n",
    "- 这样就把神经网络的输出也变成了 一个概率分布，从而可以通过交叉：脑来计算预测的概率分布和真实答案的概率分布之间的距离了。\n",
    "\n",
    "通过概率分布 q 来表达概率分布 p 的困难程度, p 代表的是正确答案， q 代表的是预测值。交叉熵；刻画的是两个概率分布的距离，也就是说交叉熵；值越小，两个概率分布越接近。\n",
    "\n",
    "![crossentrpy.png](imgs/crossentrpy.png)\n",
    "\n",
    "TensorFlow中的函数:\n",
    " - ` tf.nn .softmax_cross_ entropy_ with_ logits`\n",
    "\n",
    "\n",
    "**回归问题解决的是对具体数值的预测**\n",
    "\n",
    "- 最常用的损失函数是均方误差\n",
    "\n",
    "$ MSE(y,y^{'})=\\frac{\\sum^{n}_{i=1}(y_i - y^{'}_{i})^{2}}{n} $\n",
    "\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.5 2.5 3. ]\n",
      " [4.  4.5 4.5]]\n",
      "[[0.        0.6931472 1.0986123]\n",
      " [1.3862944 1.609438  1.7917595]]\n"
     ]
    }
   ],
   "source": [
    "#对于公式的表示解析\n",
    "# cross_entropy = -tf.reduce_mean(\n",
    "# y_ * tf.log(tf.clip_by_value(y,1e-10,1.0)))\n",
    "\n",
    "# *: 元素之间直接相乘\n",
    "# tf. matmul: 矩阵乘法\n",
    "\n",
    "# clip_by_value: 数值限制在一个范围之内，避免运算错误\n",
    "import tensorflow as tf\n",
    "v = tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]])\n",
    "v2 = \n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(tf.clip_by_value(v,2.5,4.5)))\n",
    "    print(sess.run(tf.log(v)))\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
